{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Mask R-CNN - Person Segmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tTKR4ClMsS_V","colab_type":"text"},"source":["Reference: https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py"]},{"cell_type":"markdown","metadata":{"id":"f5QKdqplsVpp","colab_type":"text"},"source":["### Imports"]},{"cell_type":"code","metadata":{"id":"FADzvMotorFu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":185},"executionInfo":{"status":"ok","timestamp":1575477387525,"user_tz":180,"elapsed":35950,"user":{"displayName":"ELVIS DIAS","photoUrl":"","userId":"09523268603658241223"}},"outputId":"12980f41-aea4-455c-c80e-0cac5be831b0"},"source":["import os\n","import os.path\n","from os import path\n","import sys\n","import cv2\n","import random\n","import math\n","import json\n","import re\n","import time\n","import skimage.draw\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","import matplotlib.image as mpimg\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Root directory of the project\n","ROOT_DIR = os.path.abspath(\"/content/drive/My Drive/TCC\")\n","MODA_DIR = os.path.join(ROOT_DIR, 'modanet/modanet/annotations/')\n","MODA_SEG_DIR = os.path.join(ROOT_DIR, 'Visualização/mascaras/test')\n","RCNN_DIR = os.path.join(ROOT_DIR, \"Mask_RCNN-master\")\n","# Import Mask RCNN from local folder\n","sys.path.append(RCNN_DIR)  \n","\n","from mrcnn.config import Config\n","from mrcnn import utils\n","from mrcnn import visualize\n","from mrcnn.visualize import display_images\n","import mrcnn.model as modellib\n","from mrcnn.model import log\n","\n","%matplotlib inline \n","\n","CUSTOM_DIR = os.path.join('/content/drive/My Drive/UFRN/FASHION/')\n","# Set path to balloon weights file\n","weights_path = os.path.join(ROOT_DIR, 'Mask_RCNN-master/mask_rcnn_coco.h5')\n","# Directory to save logs and trained model\n","MODEL_DIR = os.path.join(ROOT_DIR, \"Train/logs/inspect\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"YZxz-mlj2gcc","colab_type":"text"},"source":["## Functions"]},{"cell_type":"code","metadata":{"id":"YiM1AAn92jHJ","colab_type":"code","colab":{}},"source":["def get_ax(rows=1, cols=1, size=16):\n","    \"\"\"Return a Matplotlib Axes array to be used in\n","    all visualizations in the notebook. Provide a\n","    central point to control graph sizes.\n","    \n","    Adjust the size attribute to control how big to render images\n","    \"\"\"\n","    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n","    return ax\n","\n","\n","def plot_figures(figures, nrows = 1, ncols=1):\n","    \"\"\"Plot a dictionary of figures.\n","\n","    Parameters\n","    ----------\n","    figures : <title, figure> dictionary\n","    ncols : number of columns of subplots wanted in the display\n","    nrows : number of rows of subplots wanted in the figure\n","    \"\"\"\n","\n","    fig, axeslist = plt.subplots(ncols=ncols, nrows=nrows)\n","    for ind,title in zip(range(len(figures)), figures):\n","        axeslist.ravel()[ind].imshow(figures[title], cmap=plt.jet())\n","        axeslist.ravel()[ind].set_axis_off()\n","    plt.tight_layout() # optional\n","\n","\n","def make_segmentation_mask(image, mask,rois):\n","        img = image.copy()\n","        img[:,:,0] *= mask\n","        img[:,:,1] *= mask\n","        img[:,:,2] *= mask\n","        img[img[:,:,:]== 0] = 255\n","        cropped = img[rois[0]:rois[2],rois[1]:rois[3]]\n","\n","        return  cropped"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"St2-APIRyYWr","colab_type":"text"},"source":["## Mask R-CNN"]},{"cell_type":"markdown","metadata":{"id":"uFPQpGh31kAa","colab_type":"text"},"source":["### Config"]},{"cell_type":"code","metadata":{"id":"Vcm6LmqxrXqJ","colab_type":"code","colab":{}},"source":["class modanetConfig(Config):\n","    \"\"\"Configuration for training on MS COCO.\n","    Derives from the base Config class and overrides values specific\n","    to the COCO dataset.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"coco\"\n","    IMAGE_MIN_DIM = 400\n","    IMAGE_MAX_DIM = 1024\n","    IMAGES_PER_GPU = 1\n","    DETECTION_MIN_CONFIDENCE = 0.99\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 80  # COCO has 80 classes    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DhafIDxIzBvb","colab_type":"text"},"source":["### Modanet Dataset"]},{"cell_type":"code","metadata":{"id":"_A_L1s2GzDzh","colab_type":"code","colab":{}},"source":["class ModaNetDataset(utils.Dataset):\n","    \"\"\"Generates the modanet synthetic dataset. \"\"\"\n","    \n","    def load_moda(self, dataset_dir, subset):\n","        '''\n","        Loads a subset of the Paperdoll dataset, reads the JSON file,\n","        extracts the annotations and iteratively calls the internal add_class \n","        and add_image functions to build the dataset for the model.\n","        \n","        Args:\n","          dataset_dir: Root directory of the dataset.\n","          subset: Subset to load: train or val\n","        '''\n","        self.add_class(\"coco\", 1, \"person\")\n","        self.add_class(\"coco\", 2, \"bicycle\")\n","        self.add_class(\"coco\", 3, \"car\")\n","        self.add_class(\"coco\", 4, \"motorcycle\")\n","        self.add_class(\"coco\", 5, \"airplane\")\n","        self.add_class(\"coco\", 6, \"bus\")\n","        self.add_class(\"coco\", 7, \"train\")\n","        self.add_class(\"coco\", 8, \"truck\")\n","        self.add_class(\"coco\", 9, \"boat\")\n","        self.add_class(\"coco\", 10, \"traffic light\")\n","        self.add_class(\"coco\", 11, \"fire hydrant\")\n","        self.add_class(\"coco\", 12, \"stop sign\")\n","        self.add_class(\"coco\", 13, \"parking meter\")\n","        self.add_class(\"coco\", 14, \"bench\")\n","        self.add_class(\"coco\", 15, \"bird\")\n","        self.add_class(\"coco\", 16, \"cat\")\n","        self.add_class(\"coco\", 17, \"dog\")\n","        self.add_class(\"coco\", 18, \"horse\")\n","        self.add_class(\"coco\", 19, \"boat\")\n","        self.add_class(\"coco\", 20, \"sheep\")\n","        self.add_class(\"coco\", 21, \"cow\")\n","        self.add_class(\"coco\", 22, \"elepahant\")\n","        self.add_class(\"coco\", 23, \"bear\")\n","        self.add_class(\"coco\", 24, \"zebra\")\n","        self.add_class(\"coco\", 25, \"giraffe\")\n","        self.add_class(\"coco\", 26, \"backpack\")\n","        self.add_class(\"coco\", 27, \"umbrella\")\n","        self.add_class(\"coco\", 28, \"handbag\")\n","        self.add_class(\"coco\", 29, \"tie\")\n","        self.add_class(\"coco\", 30, \"suitcase\")\n","        self.add_class(\"coco\", 31, \"frisbee\")\n","        self.add_class(\"coco\", 32, \"skis\")\n","        self.add_class(\"coco\", 33, \"snowboard\")\n","        self.add_class(\"coco\", 34, \"sports ball\")\n","        self.add_class(\"coco\", 35, \"kite\")\n","        self.add_class(\"coco\", 36, \"baseball bat\")\n","        self.add_class(\"coco\", 37, \"baseball glove\")\n","        self.add_class(\"coco\", 38, \"skateboard\")\n","        self.add_class(\"coco\", 39, \"surfboard\")\n","        self.add_class(\"coco\", 40, \"tennis racket\")\n","        self.add_class(\"coco\", 41, \"bottle\")\n","        self.add_class(\"coco\", 42, \"wine glass\")\n","        self.add_class(\"coco\", 43, \"cup\")\n","        self.add_class(\"coco\", 44, \"fork\")\n","        self.add_class(\"coco\", 45, \"knife\")\n","        self.add_class(\"coco\", 46, \"spoon\")\n","        self.add_class(\"coco\", 47, \"bowl\")\n","        self.add_class(\"coco\", 48, \"banana\")\n","        self.add_class(\"coco\", 49, \"apple\")\n","        self.add_class(\"coco\", 50, \"sandwich\")\n","        self.add_class(\"coco\", 51, \"orange\")\n","        self.add_class(\"coco\", 52, \"broccoli\")\n","        self.add_class(\"coco\", 53, \"carrot\")\n","        self.add_class(\"coco\", 54, \"hot dog\")\n","        self.add_class(\"coco\", 55, \"pizza\")\n","        self.add_class(\"coco\", 56, \"donut\")\n","        self.add_class(\"coco\", 57, \"cake\")\n","        self.add_class(\"coco\", 58, \"chair\")\n","        self.add_class(\"coco\", 59, \"couch\")\n","        self.add_class(\"coco\", 60, \"potted plant\")\n","        self.add_class(\"coco\", 61, \"bed\")\n","        self.add_class(\"coco\", 62, \"dining table\")\n","        self.add_class(\"coco\", 63, \"toilet\")\n","        self.add_class(\"coco\", 64, \"tv\")\n","        self.add_class(\"coco\", 65, \"laptop\")\n","        self.add_class(\"coco\", 66, \"mouse\")\n","        self.add_class(\"coco\", 67, \"remote\")\n","        self.add_class(\"coco\", 68, \"keyboard\")\n","        self.add_class(\"coco\", 69, \"cell phone\")\n","        self.add_class(\"coco\", 70, \"microwave\")\n","        self.add_class(\"coco\", 71, \"over\")\n","        self.add_class(\"coco\", 72, \"toaster\")\n","        self.add_class(\"coco\", 73, \"sink\")\n","        self.add_class(\"coco\", 74, \"refrigerator\")\n","        self.add_class(\"coco\", 75, \"book\")\n","        self.add_class(\"coco\", 76, \"clock\")\n","        self.add_class(\"coco\", 77, \"vase\")\n","        self.add_class(\"coco\", 78, \"scissors\")\n","        self.add_class(\"coco\", 79, \"teddy bear\")\n","        self.add_class(\"coco\", 80, \"hair drier\")\n","        self.add_class(\"coco\", 81, \"toothbrush\")\n","        \n","        assert subset in [\"train\", \"test\", \"val\"]\n","        dataset_dir = os.path.join(dataset_dir, subset)\n","        jsonall = json.load(open(os.path.join(dataset_dir, \"instances.json\")))\n","        values = list(jsonall.values())  \n","        class_nums = []\n","        polygons = []\n","        image_ids = []\n","        category = []\n","        polygons_list = []\n","        same_img = []\n","        localid=0\n","        cont_same = 0\n","        \n","        # go through annotations JSON to get the needed values to apply on add_image()\n","        for a in values[2]:\n","            polygons.append(a['segmentation']) \n","            category.append(a['category_id']) \n","            if localid != a['image_id']: \n","                image_ids.append(a['image_id']) \n","                localid = a['image_id']\n","                same_img.append(cont_same)\n","                cont_same = 1\n","            else:\n","                cont_same +=1\n","                        \n","        same_img.append(cont_same)\n","        same_img.pop(0)\n","        \n","        for i in range(len(image_ids)):\n","          path = image_ids[i]          \n","          image_path = os.path.join(dataset_dir, str(path) + '.jpg')\n","          for j in range(same_img[i]):\n","            class_nums.append(category.pop(0))\n","            polygons_list.append(polygons.pop(0))              \n","          self.add_image('moda',\n","                          image_id = path,\n","                          path = image_path,\n","                          width = 400,\n","                          height = 600,\n","                          polygons = polygons_list,\n","                          class_list = np.array(class_nums))\n","          class_nums=[]\n","          polygons_list=[]\n","\n","          \n","    def load_mask(self, image_id):\n","        \"\"\"Generate instance masks for an image for every object in the image\n","           by drawing the polygons.\n","           \n","           Args:\n","            image_id: the image loaded and annotation loaded by load_moda\n","           Returns:\n","               masks: A bool array of shape [height, width, instance count] with\n","                      one mask per instance.\n","               class_ids: a 1D array of class IDs of the instance masks.\n","        \"\"\"\n","        class_ids = []\n","        polyx = []\n","        polyy = []\n","        polygon=[]\n","        x=[]\n","        y=[]\n","        cont = 0\n","        cont2=0\n","        cont3=0\n","        info = self.image_info[image_id]\n","        \n","        for value in info[\"polygons\"]:\n","          if (len(value) < 2):\n","            for number in value:\n","              for item in number:\n","                if cont%2 == 0:\n","                  x.append(item)\n","                else:\n","                  y.append(item)\n","                cont +=1\n","            polyx.append(x)\n","            x=[]\n","            polyy.append(y)\n","            y=[]\n","          else:\n","            for valor in value[0]:\n","              if cont2%2 == 0:\n","                x.append(valor)\n","              else:\n","                y.append(valor)\n","              cont2+=1\n","            for valor2 in value[1]:\n","              if cont3%2 == 0:\n","                x.append(valor2)\n","              else:\n","                y.append(valor2)\n","              cont3+=1\n","            polyx.append(x)\n","            x=[]\n","            polyy.append(y)\n","            y=[]\n","        \n","        mask = np.zeros([info[\"height\"], info[\"width\"], len(polyx)], dtype=np.uint8)\n","        \n","        for i, p in enumerate(polyx):\n","          rr, cc = skimage.draw.polygon(polyy[i], polyx[i]) \n","          mask[rr, cc, i] = 1\n","        class_array = info['class_list']\n","        return mask.astype(np.bool), class_array\n","\n","\n","    def image_reference(self, image_id):\n","        \"\"\"\n","        Return the path of the image: a string that identifies the image\n","        for debugging purposes\n","        \"\"\"\n","        info = self.image_info[image_id]\n","        if info[\"source\"] == \"moda\":\n","            return info[\"path\"]\n","        else:\n","            super(self.__class__, self).image_reference(image_id)     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKyOenXaE_n0","colab_type":"text"},"source":["###Eval"]},{"cell_type":"code","metadata":{"id":"efcm9nl5JWOI","colab_type":"code","colab":{}},"source":["dataset = ModaNetDataset()\n","dataset.load_moda(MODA_DIR, \"test\")\n","dataset.prepare()\n","#print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IoIRQrB6QOs3","colab_type":"code","colab":{}},"source":["config = modanetConfig()\n","#config.display()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB1d0BhkLS_p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":680},"executionInfo":{"status":"ok","timestamp":1575477448400,"user_tz":180,"elapsed":47036,"user":{"displayName":"ELVIS DIAS","photoUrl":"","userId":"09523268603658241223"}},"outputId":"e96c66b5-d9ef-4aab-cee2-438c9c16273e"},"source":["model = modellib.MaskRCNN(mode=\"inference\",\n","                          model_dir = MODEL_DIR,\n","                          config = config)\n","# Load weights\n","# utils.download_trained_weights(COCO_MODEL_PATH)\n","weights_path = os.path.join(ROOT_DIR, 'mask_rcnn_coco.h5')\n","model.load_weights(weights_path, by_name=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2139: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:399: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/drive/My Drive/TCC/Mask_RCNN-master/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s7UqdXReMIDT","colab_type":"code","colab":{}},"source":["'''\n","image_id = random.choice(dataset.image_ids)\n","image, image_meta, gt_class_id, gt_bbox, gt_mask = modellib.load_image_gt(dataset,\n","                                                                          config,\n","                                                                          image_id,\n","                                                                          use_mini_mask=False)\n","info = dataset.image_info[image_id]\n","print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"],\n","                                       info[\"id\"],\n","                                       image_id,\n","                                       dataset.image_reference(image_id)))\n","                        \n","\n","#img_list = ['49.jpg', '35.jpg', '117.jpg', '127.jpg']\n","\n","#img_list = np.random.choice(dataset.image_ids, 3)\n","#MODA_TEST = os.path.join(MODA_DIR, \"test/\")\n","FASHION_TEST = os.path.join(ROOT_DIR, \"fashion/\")\n","\n","for img in os.listdir(FASHION_TEST):\n","#for img in img_list:\n","    # n = dataset.image_reference(img)\n","    # print(n)\n","    # image = mpimg.imread(os.path.join(MODA_TEST, str(n)))\n","    image = mpimg.imread(os.path.join(FASHION_TEST, img))\n","\n","    # Run object detection\n","    results = model.detect([image], verbose=1)\n","\n","    # Display results\n","    ax = get_ax(1)\n","    r = results[0]\n","    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n","                                dataset.class_names, r['scores'], ax=ax, title=\"\")\n","'''               "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnAL6A0QUXng","colab_type":"text"},"source":["###Create Mask"]},{"cell_type":"code","metadata":{"id":"kEBYp9RBWNoW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1FQQQiyg5QiQaHIbeCOHCRKl3Q0I9PhCB"},"executionInfo":{"status":"ok","timestamp":1575483312588,"user_tz":180,"elapsed":40261,"user":{"displayName":"ELVIS DIAS","photoUrl":"","userId":"09523268603658241223"}},"outputId":"69b134f4-847b-472c-c8ee-579d3c544449"},"source":["#img_list = np.random.choice(dataset.image_ids, 200)        \n","#img_list = ['49.jpg', '35.jpg', '117.jpg', '127.jpg']\n","FASHION_TEST = os.path.join(ROOT_DIR, \"fashion/\")\n","#for img in img_list:\n","for img in os.listdir(FASHION_TEST):\n","    #n = dataset.image_reference(img)\n","    #image = mpimg.imread(os.path.join(CUSTOM_DIR, str(n)))\n","    cont=0\n","    if ~path.exists('/content/drive/My Drive/TCC/Visualização/mascaras/moda/pessoa/'+str(img)+'.png'):\n","      image = mpimg.imread(os.path.join(FASHION_TEST, img))\n","      #image = mpimg.imread(str(n))\n","      # Run object detection\n","      results = model.detect([image], verbose=1)\n","      # Display results\n","      ax = get_ax(1)\n","      r = results[0]\n","      class_ids = r['class_ids']   \n","      figures ={}\n","      for i in range(r['masks'].shape[-1]):\n","          mask = r['masks'][:,:,i]\n","          croped_roi_image = make_segmentation_mask(image, mask, r['rois'][i])\n","          figures[i] = croped_roi_image\n","          mpimg.imsave('/content/drive/My Drive/TCC/Visualização/mascaras/moda/pessoa/'+str(img)+str(cont)+'.png', figures[i])        \n","          cont+=1\n","      plot_figures(figures, 1, 6)\n","      visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], dataset.class_names, r['scores'], ax=ax, title=\"\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}